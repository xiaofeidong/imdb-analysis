{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yy/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/yy/anaconda/lib/python3.6/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import mysql.connector\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.data\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from sklearn import naive_bayes, svm, preprocessing\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection.univariate_selection import chi2, SelectKBest\n",
    "from ipywidgets import interact, FloatSlider, Select, Dropdown, widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/yy/Documents/Dongxf/liu/sentiment-analysis-master\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = mysql.connector.connect(host=\"localhost\",\n",
    "                     user=\"root\",\n",
    "                     passwd=\"850718\",\n",
    "                     db=\"oscar\", use_unicode=True, charset=\"utf8\") # name of the data base\n",
    "cur = db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_csv = True\n",
    "# term_vector_type = {\"TFIDF\", \"Binary\", \"Int\", \"Word2vec\", \"Word2vec_pretrained\"}\n",
    "# {\"TFIDF\", \"Int\", \"Binary\"}: Bag-of-words model with {tf-idf, word counts, presence/absence} representation\n",
    "# {\"Word2vec\", \"Word2vec_pretrained\"}: Google word2vec representation {without, with} pre-trained models\n",
    "# Specify model_name if there's a pre-trained model to be loaded\n",
    "vector_type = \"TFIDF\"\n",
    "model_name = \"GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "# model_type = {\"bin\", \"reg\"}\n",
    "# Specify whether pre-trained word2vec model is binary\n",
    "model_type = \"bin\"\n",
    "   \n",
    "# Parameters for word2vec\n",
    "# num_features need to be identical with the pre-trained model\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count to be included for training                      \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# training_model = {\"RF\", \"NB\", \"SVM\", \"BT\", \"no\"}\n",
    "training_model = \"SVM\"\n",
    "\n",
    "# feature scaling = {\"standard\", \"signed\", \"unsigned\", \"no\"}\n",
    "# Note: Scaling is needed for SVM\n",
    "scaling = \"standard\"\n",
    "\n",
    "# dimension reduction = {\"SVD\", \"chi2\", \"no\"}\n",
    "# Note: For NB models, we cannot perform truncated SVD as it will make input negative\n",
    "# chi2 is the feature selectioin based on chi2 independence test\n",
    "dim_reduce = \"chi2\"\n",
    "num_dim = 500\n",
    "\n",
    "##################### End of Initialization #####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_review(raw_review, remove_stopwords = False, output_format = \"string\"):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "            raw_review: raw text of a movie review\n",
    "            remove_stopwords: a boolean variable to indicate whether to remove stop words\n",
    "            output_format: if \"string\", return a cleaned string \n",
    "                           if \"list\", a list of words extracted from cleaned string.\n",
    "    Output:\n",
    "            Cleaned string or list.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove HTML markup\n",
    "    text = BeautifulSoup(raw_review, \"lxml\")\n",
    "    \n",
    "    # Keep only characters\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text.get_text())\n",
    "    \n",
    "    # Split words and store to list\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "    \n",
    "        # Use set as it has O(1) lookup time\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in text if w not in stops]\n",
    "    \n",
    "    else:\n",
    "        words = text\n",
    "    \n",
    "    # Return a cleaned string or list\n",
    "    if output_format == \"string\":\n",
    "        return \" \".join(words)\n",
    "        \n",
    "    elif output_format == \"list\":\n",
    "        return words\n",
    "    \n",
    "    \n",
    "def review_to_doublelist(review, tokenizer, remove_stopwords = False):\n",
    "    \"\"\"\n",
    "    Function which generates a list of lists of words from a review for word2vec uses.\n",
    "    \n",
    "    Input:\n",
    "        review: raw text of a movie review\n",
    "        tokenizer: tokenizer for sentence parsing\n",
    "                   nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        remove_stopwords: a boolean variable to indicate whether to remove stop words\n",
    "    \n",
    "    Output:\n",
    "        A list of lists.\n",
    "        The outer list consists of all sentences in a review.\n",
    "        The inner list consists of all words in a sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a list of sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentence_list = []\n",
    "    \n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentence_list.append(clean_review(raw_sentence, False, \"list\"))         \n",
    "    return sentence_list\n",
    "\n",
    "\n",
    "def review_to_vec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    Function which generates a feature vector for the given review.\n",
    "    \n",
    "    Input:\n",
    "        words: a list of words extracted from a review\n",
    "        model: trained word2vec model\n",
    "        num_features: dimension of word2vec vectors\n",
    "        \n",
    "    Output:\n",
    "        a numpy array representing the review\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_vec = np.zeros((num_features), dtype=\"float32\")\n",
    "    word_count = 0\n",
    "    \n",
    "    # index2word is a list consisting of all words in the vocabulary\n",
    "    # Convert list to set for speed\n",
    "    index2word_set = set(model.index2word)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            word_count += 1\n",
    "            feature_vec += model[word]\n",
    "\n",
    "    feature_vec /= word_count\n",
    "    return feature_vec\n",
    "    \n",
    "    \n",
    "def gen_review_vecs(reviews, model, num_features):\n",
    "    \"\"\"\n",
    "    Function which generates a m-by-n numpy array from all reviews,\n",
    "    where m is len(reviews), and n is num_feature\n",
    "    \n",
    "    Input:\n",
    "            reviews: a list of lists. \n",
    "                     Inner lists are words from each review.\n",
    "                     Outer lists consist of all reviews\n",
    "            model: trained word2vec model\n",
    "            num_feature: dimension of word2vec vectors\n",
    "    Output: m-by-n numpy array, where m is len(review) and n is num_feature\n",
    "    \"\"\"\n",
    "\n",
    "    curr_index = 0\n",
    "    review_feature_vecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n",
    "\n",
    "    for review in reviews:\n",
    "\n",
    "       if curr_index%1000 == 0.:\n",
    "           print (\"Vectorizing review %d of %d\" % (curr_index, len(reviews)))\n",
    "   \n",
    "       review_feature_vecs[curr_index] = review_to_vec(review, model, num_features)\n",
    "       curr_index += 1\n",
    "       \n",
    "    return review_feature_vecs\n",
    "    \n",
    "    \n",
    "##################### End of Function Definition #####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "movieReviews = widgets.Text(description=\"MovieID\", width=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def  movie_reviews_sentiment(b):\n",
    "    \n",
    "    if movieReviews.value!='':\n",
    "        \n",
    "        m = movieReviews.value\n",
    "        print (m)\n",
    "        cur.execute(\"SELECT * FROM topvoted10000  where movieID=%s\"%(m))\n",
    "        row2 =  cur.fetchall()\n",
    "        movieName = []\n",
    "        for row in row2:\n",
    "            movieName.append(row[1].decode('utf-8'))\n",
    "        print (\"Movie Name:\" + movieName[0])\n",
    "\n",
    "        train_list = []\n",
    "        test_list = []\n",
    "        word2vec_input = []\n",
    "        pred = []\n",
    "\n",
    "        train_data = pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=0)\n",
    "        #test_data = pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\", quoting=0)\n",
    "\n",
    "\n",
    "        sql = cur.execute(\"SELECT movieID, authorID, reviewContent, reviewersMovieRating, movieName, reviewTitle FROM top10000_reviews_all where movieID=%s\"%(m))\n",
    "        row1 =  cur.fetchall()\n",
    "        testData_movieID = []\n",
    "        testData_authorID = []\n",
    "        testData_review = []\n",
    "        testData_rating = []\n",
    "        teseData_movieName = []\n",
    "        teseData_reviewTitle = []\n",
    "\n",
    "        for row in row1:\n",
    "            testData_movieID.append(bytes(row[0]).decode('utf-8'))\n",
    "        for row in row1:\n",
    "            testData_authorID.append(bytes(row[1]).decode('utf-8'))\n",
    "        for row in row1:\n",
    "            testData_review.append(row[2])\n",
    "        for row in row1:\n",
    "            testData_rating.append(bytes(row[3]).decode('utf-8'))\n",
    "        for row in row1:\n",
    "            teseData_movieName.append(bytes(row[4]).decode('utf-8'))\n",
    "        for row in row1:\n",
    "            teseData_reviewTitle.append(bytes(row[5]).decode('utf-8'))\n",
    "\n",
    "\n",
    "        if vector_type == \"Word2vec\":\n",
    "            unlab_train_data = pd.read_csv(\"unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "            tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "            logging.basicConfig(format='%(asctime)s: %(message)s', level=logging.INFO)\n",
    "\n",
    "        # Extract words from reviews\n",
    "        # range is faster when iterating\n",
    "        if vector_type == \"Word2vec\" or vector_type == \"Word2vec_pretrained\":\n",
    "\n",
    "            for i in range(0, len(train_data.review)):\n",
    "\n",
    "                if vector_type == \"Word2vec\":\n",
    "                    # Decode utf-8 coding first\n",
    "                    word2vec_input.extend(review_to_doublelist(train_data.review[i], tokenizer))\n",
    "\n",
    "                train_list.append(clean_review(train_data.review[i], output_format=\"list\"))\n",
    "                if i%10000 == 0:\n",
    "                    print (\"Cleaning training review\", i)\n",
    "\n",
    "            if vector_type == \"Word2vec\":\n",
    "                for i in range(0, len(unlab_train_data.review)):\n",
    "                    word2vec_input.extend(review_to_doublelist(unlab_train_data.review[i], tokenizer))\n",
    "                    if i%1000 == 0:\n",
    "                        print (\"Cleaning unlabeled training review\", i)\n",
    "\n",
    "            for i in range(0, len(testData_review)):\n",
    "                test_list.append(clean_review(bytes(testData_review[i]), output_format=\"list\"))\n",
    "                if i%10000 == 0:\n",
    "                    print (\"Cleaning test review\", i)\n",
    "\n",
    "        elif vector_type != \"no\":\n",
    "            for i in range(0, len(train_data.review)):\n",
    "\n",
    "                # Append raw texts rather than lists as Count/TFIDF vectorizers take raw texts as inputs\n",
    "                train_list.append(clean_review(train_data.review[i]))\n",
    "                if i%10000 == 0:\n",
    "                    print (\"Cleaning training review\", i)\n",
    "\n",
    "            for i in range(0, len(testData_review)):\n",
    "\n",
    "                # Append raw texts rather than lists as Count/TFIDF vectorizers take raw texts as inputs\n",
    "                test_list.append(clean_review(bytes(testData_review[i])))\n",
    "                if i%10000 == 0:\n",
    "                    print (\"Cleaning test review\", i)\n",
    "\n",
    "\n",
    "\n",
    "        # Generate vectors from words\n",
    "        if vector_type == \"Word2vec_pretrained\" or vector_type == \"Word2vec\":\n",
    "\n",
    "            if vector_type == \"Word2vec_pretrained\":\n",
    "                print (\"Loading the pre-trained model\")\n",
    "                if model_type == \"bin\":\n",
    "                    model = word2vec.Word2Vec.load_word2vec_format(model_name, binary=True)\n",
    "                else:\n",
    "                    model = word2vec.Word2Vec.load(model_name)\n",
    "\n",
    "            if vector_type == \"Word2vec\":\n",
    "                print (\"Training word2vec word vectors\")\n",
    "                model = word2vec.Word2Vec(word2vec_input, workers=num_workers, \\\n",
    "                                        size=num_features, min_count = min_word_count, \\\n",
    "                                        window = context, sample = downsampling)\n",
    "\n",
    "                # If no further training and only query is needed, this trims unnecessary memory\n",
    "                model.init_sims(replace=True)\n",
    "\n",
    "                # Save the model for later use\n",
    "                model.save(model_name)\n",
    "\n",
    "            print (\"Vectorizing training review\")\n",
    "            train_vec = gen_review_vecs(train_list, model, num_features)\n",
    "            print (\"Vectorizing test review\")\n",
    "            test_vec = gen_review_vecs(test_list, model, num_features)\n",
    "\n",
    "\n",
    "        elif vector_type != \"no\":\n",
    "            if vector_type == \"TFIDF\":\n",
    "                # Unit of gram is \"word\", only top 5000/10000 words are extracted\n",
    "                count_vec = TfidfVectorizer(analyzer=\"word\", max_features=10000, ngram_range=(1,2), sublinear_tf=True)\n",
    "\n",
    "            elif vector_type == \"Binary\" or vector_type == \"Int\":\n",
    "                count_vec = CountVectorizer(analyzer=\"word\", max_features=10000, \\\n",
    "                                            binary = (vector_type == \"Binary\"), \\\n",
    "                                            ngram_range=(1,2))\n",
    "\n",
    "            # Return a scipy sparse term-document matrix\n",
    "            print (\"Vectorizing input texts\")\n",
    "            train_vec = count_vec.fit_transform(train_list)\n",
    "            test_vec = count_vec.transform(test_list)\n",
    "        # Dimemsion Reduction\n",
    "        if dim_reduce == \"SVD\":\n",
    "            print (\"Performing dimension reduction\")\n",
    "            svd = TruncatedSVD(n_components = num_dim)\n",
    "            train_vec = svd.fit_transform(train_vec)\n",
    "            test_vec = svd.transform(test_vec)\n",
    "            print (\"Explained variance ratio =\", svd.explained_variance_ratio_.sum())\n",
    "\n",
    "        elif dim_reduce == \"chi2\":\n",
    "            print (\"Performing feature selection based on chi2 independence test\")\n",
    "            fselect = SelectKBest(chi2, k=num_dim)\n",
    "            train_vec = fselect.fit_transform(train_vec, train_data.sentiment)\n",
    "            test_vec = fselect.transform(test_vec)\n",
    "\n",
    "        # Transform into numpy arrays\n",
    "        if \"numpy.ndarray\" not in str(type(train_vec)):\n",
    "            train_vec = train_vec.toarray()\n",
    "            test_vec = test_vec.toarray()\n",
    "\n",
    "        # Feature Scaling\n",
    "        if scaling != \"no\":\n",
    "\n",
    "            if scaling == \"standard\":\n",
    "                scaler = preprocessing.StandardScaler()\n",
    "            else:\n",
    "                if scaling == \"unsigned\":\n",
    "                    scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "                elif scaling == \"signed\":\n",
    "                    scaler = preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "            print (\"Scaling vectors\")\n",
    "            train_vec = scaler.fit_transform(train_vec)\n",
    "            test_vec = scaler.transform(test_vec)\n",
    "\n",
    "        # Model training\n",
    "        if training_model == \"RF\" or training_model == \"BT\":\n",
    "\n",
    "            # Initialize the Random Forest or bagged tree based the model chosen\n",
    "            rfc = RFC(n_estimators = 100, oob_score = True, \\\n",
    "                      max_features = (None if training_model==\"BT\" else \"auto\"))\n",
    "            print (\"Training %s\" % (\"Random Forest\" if training_model==\"RF\" else \"bagged tree\"))\n",
    "            rfc = rfc.fit(train_vec, train_data.sentiment)\n",
    "            print (\"OOB Score =\", rfc.oob_score_)\n",
    "            pred = rfc.predict(test_vec)\n",
    "\n",
    "        elif training_model == \"NB\":\n",
    "            nb = naive_bayes.MultinomialNB()\n",
    "            cv_score = cross_val_score(nb, train_vec, train_data.sentiment, cv=10)\n",
    "            print (\"Training Naive Bayes\")\n",
    "            print (\"CV Score = \", cv_score.mean())\n",
    "            nb = nb.fit(train_vec, train_data.sentiment)\n",
    "            pred = nb.predict(test_vec)\n",
    "\n",
    "        elif training_model == \"SVM\":\n",
    "            svc = svm.LinearSVC()\n",
    "            param = {'C': [1e15,1e13,1e11,1e9,1e7,1e5,1e3,1e1,1e-1,1e-3,1e-5]}\n",
    "            print (\"Training SVM\")\n",
    "            svc = GridSearchCV(svc, param, cv=10)\n",
    "            svc = svc.fit(train_vec, train_data.sentiment)\n",
    "            pred = svc.predict(test_vec)\n",
    "            print (\"Optimized parameters:\", svc.best_estimator_)\n",
    "            print (\"Best CV score:\", svc.best_score_)\n",
    "\n",
    "\n",
    "        # Output the results\n",
    "        if write_to_csv:\n",
    "            length = len(testData_movieID)\n",
    "            print (\"Total Reviews: \" , length)\n",
    "            pred1 = pred.astype(int)\n",
    "            for i in range(0, length):\n",
    "                cur.execute('''INSERT into submission_copy (movieID, authorID, movieName, reviewTitle, rating, sentiment) values (%s, %s, %s, %s, %s, %s)''',(testData_movieID[i], testData_authorID[i], teseData_movieName[i], teseData_reviewTitle[i], testData_rating[i], int(pred1[i])))\n",
    "                db.commit()\n",
    "            cur.close\n",
    "            db.close\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review_text = widgets.Text(description=\"Input\", width=200)\n",
    "def  input_review_sentiment(b):\n",
    "    \n",
    "    if review_text.value!='':\n",
    "        \n",
    "        m = review_text.value\n",
    "        #cur.execute(\"SELECT * FROM topvoted10000 LIMIT %s,1\"%(m))\n",
    "        #row2 =  cur.fetchall()\n",
    "        #col1 = []\n",
    "        #for row in row2:\n",
    "        #    col1.append(row[0].decode('utf-8'))\n",
    "\n",
    "        train_list = []\n",
    "        test_list = []\n",
    "        word2vec_input = []\n",
    "        pred = []\n",
    "\n",
    "        train_data = pd.read_csv(\"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=0)\n",
    "        #test_data = pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\", quoting=0)\n",
    "\n",
    "        testData_review = []\n",
    "        testData_review.append(m)\n",
    "        print (testData_review)\n",
    "\n",
    "\n",
    "        if vector_type == \"Word2vec\":\n",
    "            unlab_train_data = pd.read_csv(\"unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "            tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "            logging.basicConfig(format='%(asctime)s: %(message)s', level=logging.INFO)\n",
    "\n",
    "        # Extract words from reviews\n",
    "        # range is faster when iterating\n",
    "        if vector_type == \"Word2vec\" or vector_type == \"Word2vec_pretrained\":\n",
    "\n",
    "            for i in range(0, len(train_data.review)):\n",
    "\n",
    "                if vector_type == \"Word2vec\":\n",
    "                    # Decode utf-8 coding first\n",
    "                    word2vec_input.extend(review_to_doublelist(train_data.review[i], tokenizer))\n",
    "\n",
    "                train_list.append(clean_review(train_data.review[i], output_format=\"list\"))\n",
    "                if i%1000 == 0:\n",
    "                    print (\"Cleaning training review\", i)\n",
    "\n",
    "            if vector_type == \"Word2vec\":\n",
    "                for i in range(0, len(unlab_train_data.review)):\n",
    "                    word2vec_input.extend(review_to_doublelist(unlab_train_data.review[i], tokenizer))\n",
    "                    if i%1000 == 0:\n",
    "                        print (\"Cleaning unlabeled training review\", i)\n",
    "\n",
    "            for i in range(0, len(testData_review)):\n",
    "                test_list.append(clean_review(testData_review[i], output_format=\"list\"))\n",
    "                if i%1000 == 0:\n",
    "                    print (\"Cleaning test review\", i)\n",
    "\n",
    "        elif vector_type != \"no\":\n",
    "            for i in range(0, len(train_data.review)):\n",
    "\n",
    "                # Append raw texts rather than lists as Count/TFIDF vectorizers take raw texts as inputs\n",
    "                train_list.append(clean_review(train_data.review[i]))\n",
    "                if i%1000 == 0:\n",
    "                    print (\"Cleaning training review\", i)\n",
    "\n",
    "            for i in range(0, len(testData_review)):\n",
    "\n",
    "                # Append raw texts rather than lists as Count/TFIDF vectorizers take raw texts as inputs\n",
    "                test_list.append(clean_review(testData_review[i]))\n",
    "                if i%1000 == 0:\n",
    "                    print (\"Cleaning test review\", i)\n",
    "\n",
    "\n",
    "\n",
    "        # Generate vectors from words\n",
    "        if vector_type == \"Word2vec_pretrained\" or vector_type == \"Word2vec\":\n",
    "\n",
    "            if vector_type == \"Word2vec_pretrained\":\n",
    "                print (\"Loading the pre-trained model\")\n",
    "                if model_type == \"bin\":\n",
    "                    model = word2vec.Word2Vec.load_word2vec_format(model_name, binary=True)\n",
    "                else:\n",
    "                    model = word2vec.Word2Vec.load(model_name)\n",
    "\n",
    "            if vector_type == \"Word2vec\":\n",
    "                print (\"Training word2vec word vectors\")\n",
    "                model = word2vec.Word2Vec(word2vec_input, workers=num_workers, \\\n",
    "                                        size=num_features, min_count = min_word_count, \\\n",
    "                                        window = context, sample = downsampling)\n",
    "\n",
    "                # If no further training and only query is needed, this trims unnecessary memory\n",
    "                model.init_sims(replace=True)\n",
    "\n",
    "                # Save the model for later use\n",
    "                model.save(model_name)\n",
    "\n",
    "            print (\"Vectorizing training review\")\n",
    "            train_vec = gen_review_vecs(train_list, model, num_features)\n",
    "            print (\"Vectorizing test review\")\n",
    "            test_vec = gen_review_vecs(test_list, model, num_features)\n",
    "\n",
    "\n",
    "        elif vector_type != \"no\":\n",
    "            if vector_type == \"TFIDF\":\n",
    "                # Unit of gram is \"word\", only top 5000/10000 words are extracted\n",
    "                count_vec = TfidfVectorizer(analyzer=\"word\", max_features=10000, ngram_range=(1,2), sublinear_tf=True)\n",
    "\n",
    "            elif vector_type == \"Binary\" or vector_type == \"Int\":\n",
    "                count_vec = CountVectorizer(analyzer=\"word\", max_features=10000, \\\n",
    "                                            binary = (vector_type == \"Binary\"), \\\n",
    "                                            ngram_range=(1,2))\n",
    "\n",
    "            # Return a scipy sparse term-document matrix\n",
    "            print (\"Vectorizing input texts\")\n",
    "            train_vec = count_vec.fit_transform(train_list)\n",
    "            test_vec = count_vec.transform(test_list)\n",
    "        # Dimemsion Reduction\n",
    "        if dim_reduce == \"SVD\":\n",
    "            print (\"Performing dimension reduction\")\n",
    "            svd = TruncatedSVD(n_components = num_dim)\n",
    "            train_vec = svd.fit_transform(train_vec)\n",
    "            test_vec = svd.transform(test_vec)\n",
    "            print (\"Explained variance ratio =\", svd.explained_variance_ratio_.sum())\n",
    "\n",
    "        elif dim_reduce == \"chi2\":\n",
    "            print (\"Performing feature selection based on chi2 independence test\")\n",
    "            fselect = SelectKBest(chi2, k=num_dim)\n",
    "            train_vec = fselect.fit_transform(train_vec, train_data.sentiment)\n",
    "            test_vec = fselect.transform(test_vec)\n",
    "\n",
    "        # Transform into numpy arrays\n",
    "        if \"numpy.ndarray\" not in str(type(train_vec)):\n",
    "            train_vec = train_vec.toarray()\n",
    "            test_vec = test_vec.toarray()\n",
    "\n",
    "        # Feature Scaling\n",
    "        if scaling != \"no\":\n",
    "\n",
    "            if scaling == \"standard\":\n",
    "                scaler = preprocessing.StandardScaler()\n",
    "            else:\n",
    "                if scaling == \"unsigned\":\n",
    "                    scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "                elif scaling == \"signed\":\n",
    "                    scaler = preprocessing.MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "            print (\"Scaling vectors\")\n",
    "            train_vec = scaler.fit_transform(train_vec)\n",
    "            test_vec = scaler.transform(test_vec)\n",
    "\n",
    "        # Model training\n",
    "        if training_model == \"RF\" or training_model == \"BT\":\n",
    "\n",
    "            # Initialize the Random Forest or bagged tree based the model chosen\n",
    "            rfc = RFC(n_estimators = 100, oob_score = True, \\\n",
    "                      max_features = (None if training_model==\"BT\" else \"auto\"))\n",
    "            print (\"Training %s\" % (\"Random Forest\" if training_model==\"RF\" else \"bagged tree\"))\n",
    "            rfc = rfc.fit(train_vec, train_data.sentiment)\n",
    "            print (\"OOB Score =\", rfc.oob_score_)\n",
    "            pred = rfc.predict(test_vec)\n",
    "\n",
    "        elif training_model == \"NB\":\n",
    "            nb = naive_bayes.MultinomialNB()\n",
    "            cv_score = cross_val_score(nb, train_vec, train_data.sentiment, cv=10)\n",
    "            print (\"Training Naive Bayes\")\n",
    "            print (\"CV Score = \", cv_score.mean())\n",
    "            nb = nb.fit(train_vec, train_data.sentiment)\n",
    "            pred = nb.predict(test_vec)\n",
    "\n",
    "        elif training_model == \"SVM\":\n",
    "            svc = svm.LinearSVC()\n",
    "            param = {'C': [1e15,1e13,1e11,1e9,1e7,1e5,1e3,1e1,1e-1,1e-3,1e-5]}\n",
    "            print (\"Training SVM\")\n",
    "            svc = GridSearchCV(svc, param, cv=10)\n",
    "            svc = svc.fit(train_vec, train_data.sentiment)\n",
    "            pred = svc.predict(test_vec)\n",
    "            print (\"Optimized parameters:\", svc.best_estimator_)\n",
    "            print (\"Best CV score:\", svc.best_score_)\n",
    "\n",
    "\n",
    "        # Output the results\n",
    "        if write_to_csv:\n",
    "            pred1 = pred.astype(int)\n",
    "            print (pred1)\n",
    "            cur.close\n",
    "            db.close\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(review_text, width=200)\n",
    "button = widgets.Button(description=\"Review Sentiment\")\n",
    "display(button)\n",
    "    \n",
    "button.on_click(input_review_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0024894\n",
      "Movie Name:The Black Cat\n",
      "Cleaning training review 0\n",
      "Cleaning training review 10000\n",
      "Cleaning training review 20000\n",
      "Cleaning test review 0\n",
      "Vectorizing input texts\n"
     ]
    }
   ],
   "source": [
    "display(movieReviews)\n",
    "button = widgets.Button(description=\"Sentiment Analysis\")\n",
    "display(button)\n",
    "   \n",
    "button.on_click(movie_reviews_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur.execute(\"SELECT rating, COUNT(CASE WHEN sentiment = '0'  THEN 1 ELSE NULL END) AS negative ,COUNT(CASE WHEN sentiment = '1' THEN 1 ELSE NULL END) AS postive FROM submission group by rating \")\n",
    "row4 =  cur.fetchall()\n",
    "review_rating = []\n",
    "negative = []\n",
    "postive = []\n",
    "#genre\n",
    "for row in row4:\n",
    "    review_rating.append(row[0])\n",
    "    negative.append(int(row[1]))\n",
    "    postive.append(int(row[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12e1deb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnMAAAJMCAYAAACRnCB2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c056ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sea\n",
    "\n",
    "sea.set_style('dark')\n",
    "\n",
    "fig5 = plt.figure()\n",
    "\n",
    "review_rating = np.asarray(review_rating)\n",
    "negative = np.asarray(negative)\n",
    "postive = np.asarray(postive)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'review_rating':review_rating.tolist(),\n",
    "    'negative':negative.tolist(),\n",
    "    'postive':postive.tolist()\n",
    "})\n",
    "tidy = (\n",
    "    df.set_index('review_rating')\n",
    "      .stack()  # un-pivots the data \n",
    "      .reset_index()  # moves all data out of the index\n",
    "      .rename(columns={'level_1': 'Variable', 0: 'Value'})\n",
    ")\n",
    "fig, ax1 = plt.subplots(figsize=(10, 10))\n",
    "sea.barplot(x='review_rating', y='Value', hue='Variable', data=tidy, ax=ax1)\n",
    "sea.despine(fig)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "2ae63eefbee0476bac6d9342c56bd36d": {
     "views": [
      {
       "cell_index": 9
      }
     ]
    },
    "3edb09757d1b461bbe24e24d4dd68361": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "733bcaeca42741ab87f64a4182f39671": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "a00f3cc139414a5082a1e964ab3ec681": {
     "views": [
      {
       "cell_index": 9
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
